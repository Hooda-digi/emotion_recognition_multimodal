{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05f93f27-11a7-4161-a679-50171737c6e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3434/405325148.py:1: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n",
      "  torchaudio.set_audio_backend(\"sox_io\")\n"
     ]
    }
   ],
   "source": [
    "torchaudio.set_audio_backend(\"sox_io\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb9d2407-0a1b-4c4e-8e61-21ff54367992",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.4\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(np.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14430a22-3f52-43a7-a123-232a53bce370",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 72\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     71\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/common/ai_workspace/user_space/hooda_workspace/emotion_recognition/wave2vec2/audio_file/03-01-05-01-01-01-05.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your audio file path\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m emotion \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_emotion_with_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Emotion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[21], line 51\u001b[0m, in \u001b[0;36mpredict_emotion_with_attention\u001b[0;34m(audio_path)\u001b[0m\n\u001b[1;32m     48\u001b[0m input_audio \u001b[38;5;241m=\u001b[39m preprocess_audio(audio_path)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Tokenize audio and prepare inputs\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m inputs \u001b[38;5;241m=\u001b[39m processor(\u001b[43minput_audio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, sampling_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Debugging: print shapes of input_values and attention_mask\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_values shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minputs\u001b[38;5;241m.\u001b[39minput_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import librosa\n",
    "import numpy\n",
    "\n",
    "# Define the custom model\n",
    "class Wav2Vec2WithAttention(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_classes):\n",
    "        super(Wav2Vec2WithAttention, self).__init__()\n",
    "        # Load Wav2Vec2 base model\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(pretrained_model_name)\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.wav2vec2.config.hidden_size, num_heads=4, batch_first=True)\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(self.wav2vec2.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        # Get hidden states from Wav2Vec2\n",
    "        hidden_states = self.wav2vec2(input_values, attention_mask=attention_mask).last_hidden_state\n",
    "        # Apply self-attention\n",
    "        attention_output, _ = self.attention(hidden_states, hidden_states, hidden_states)\n",
    "        # Mean pooling\n",
    "        pooled_output = torch.mean(attention_output, dim=1)\n",
    "        # Classify\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load pre-trained Wav2Vec2 processor and custom model\n",
    "PRETRAINED_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "NUM_CLASSES = 8  # Number of emotion classes\n",
    "processor = Wav2Vec2Processor.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model = Wav2Vec2WithAttention(PRETRAINED_MODEL_NAME, NUM_CLASSES)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Preprocess audio function\n",
    "def preprocess_audio(file_path):\n",
    "    # Load the audio file using librosa\n",
    "    waveform, sample_rate = librosa.load(file_path, sr=16000)\n",
    "    return torch.tensor(waveform, dtype=torch.float32)\n",
    "\n",
    "# Predict emotion function\n",
    "def predict_emotion_with_attention(audio_path):\n",
    "    # Preprocess audio\n",
    "    input_audio = preprocess_audio(audio_path)\n",
    "    \n",
    "    # Tokenize audio and prepare inputs\n",
    "    inputs = processor(input_audio.numpy(), sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    # Debugging: print shapes of input_values and attention_mask\n",
    "    print(f\"input_values shape: {inputs.input_values.shape}\")\n",
    "    print(f\"attention_mask shape: {inputs.attention_mask.shape}\")\n",
    "\n",
    "    input_values = inputs.input_values.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    # Predict emotion\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask)\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # Emotion labels (can customize based on dataset)\n",
    "    emotion_labels = {0: \"neutral\", 1: \"happy\", 2: \"sad\", 3: \"angry\", 4: \"fearful\", 5: \"disgust\", 6: \"surprised\", 7: \"calm\"}\n",
    "    return emotion_labels.get(predicted_class, \"unknown\")\n",
    "\n",
    "# Example usage\n",
    "audio_file = \"/home/common/ai_workspace/user_space/hooda_workspace/emotion_recognition/wave2vec2/audio_file/03-01-05-01-01-01-05.wav\"  # Replace with your audio file path\n",
    "emotion = predict_emotion_with_attention(audio_file)\n",
    "print(f\"Predicted Emotion: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d300bbb3-52af-4df8-ab47-f36b53f2ddd0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Couldn't find appropriate backend to handle uri /home/common/ai_workspace/user_space/hooda_workspace/emotion_recognition/wave2vec2/audio_file/03-01-05-01-01-01-05.wav and format None.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 81\u001b[0m\n\u001b[1;32m     79\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/common/ai_workspace/user_space/hooda_workspace/emotion_recognition/wave2vec2/audio_file/03-01-05-01-01-01-05.wav\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# Replace with your audio file path\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# preprocess_audio(audio_file)\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m emotion \u001b[38;5;241m=\u001b[39m predict_emotion_with_attention(\u001b[43mpreprocess_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Emotion: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00memotion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 52\u001b[0m, in \u001b[0;36mpreprocess_audio\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess_audio\u001b[39m(file_path):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;66;03m# Load the audio file and resample to 16kHz\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m     waveform, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mtorchaudio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m sample_rate \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m16000\u001b[39m:\n\u001b[1;32m     54\u001b[0m         resample \u001b[38;5;241m=\u001b[39m torchaudio\u001b[38;5;241m.\u001b[39mtransforms\u001b[38;5;241m.\u001b[39mResample(orig_freq\u001b[38;5;241m=\u001b[39msample_rate, new_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m)\n",
      "File \u001b[0;32m~/ai_workspace/user_space/hooda_workspace/hman_emotions/emotion_rec_env/lib/python3.8/site-packages/torchaudio/_backend/utils.py:204\u001b[0m, in \u001b[0;36mget_load_func.<locals>.load\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[1;32m    119\u001b[0m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[1;32m    120\u001b[0m     frame_offset: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    126\u001b[0m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    127\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[1;32m    128\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load audio data from source.\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;03m    By default (``normalize=True``, ``channels_first=True``), this function returns Tensor with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m            `[channel, time]` else `[time, channel]`.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m     backend \u001b[38;5;241m=\u001b[39m \u001b[43mdispatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mload(uri, frame_offset, num_frames, normalize, channels_first, \u001b[38;5;28mformat\u001b[39m, buffer_size)\n",
      "File \u001b[0;32m~/ai_workspace/user_space/hooda_workspace/hman_emotions/emotion_rec_env/lib/python3.8/site-packages/torchaudio/_backend/utils.py:116\u001b[0m, in \u001b[0;36mget_load_func.<locals>.dispatcher\u001b[0;34m(uri, format, backend_name)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mcan_decode(uri, \u001b[38;5;28mformat\u001b[39m):\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m backend\n\u001b[0;32m--> 116\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCouldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find appropriate backend to handle uri \u001b[39m\u001b[38;5;132;01m{\u001b[39;00muri\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and format \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mformat\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Couldn't find appropriate backend to handle uri /home/common/ai_workspace/user_space/hooda_workspace/emotion_recognition/wave2vec2/audio_file/03-01-05-01-01-01-05.wav and format None."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import Wav2Vec2Model, Wav2Vec2Processor\n",
    "import torchaudio\n",
    "\n",
    "import soundfile as sf\n",
    "\n",
    "def preprocess_audio(file_path):\n",
    "    # Load the audio file using soundfile\n",
    "    waveform, sample_rate = sf.read(file_path)\n",
    "    if sample_rate != 16000:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resample(torch.tensor(waveform).unsqueeze(0))\n",
    "    return waveform.squeeze().numpy()\n",
    "\n",
    "\n",
    "# Define the custom model\n",
    "class Wav2Vec2WithAttention(nn.Module):\n",
    "    def __init__(self, pretrained_model_name, num_classes):\n",
    "        super(Wav2Vec2WithAttention, self).__init__()\n",
    "        # Load Wav2Vec2 base model\n",
    "        self.wav2vec2 = Wav2Vec2Model.from_pretrained(pretrained_model_name)\n",
    "        # Attention mechanism\n",
    "        self.attention = nn.MultiheadAttention(embed_dim=self.wav2vec2.config.hidden_size, num_heads=4, batch_first=True)\n",
    "        # Fully connected layer for classification\n",
    "        self.fc = nn.Linear(self.wav2vec2.config.hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_values, attention_mask=None):\n",
    "        # Get hidden states from Wav2Vec2\n",
    "        hidden_states = self.wav2vec2(input_values, attention_mask=attention_mask).last_hidden_state\n",
    "        # Apply self-attention\n",
    "        attention_output, _ = self.attention(hidden_states, hidden_states, hidden_states)\n",
    "        # Mean pooling\n",
    "        pooled_output = torch.mean(attention_output, dim=1)\n",
    "        # Classify\n",
    "        logits = self.fc(pooled_output)\n",
    "        return logits\n",
    "\n",
    "# Load pre-trained Wav2Vec2 processor and custom model\n",
    "PRETRAINED_MODEL_NAME = \"facebook/wav2vec2-base\"\n",
    "NUM_CLASSES = 8  # Number of emotion classes\n",
    "processor = Wav2Vec2Processor.from_pretrained(PRETRAINED_MODEL_NAME)\n",
    "model = Wav2Vec2WithAttention(PRETRAINED_MODEL_NAME, NUM_CLASSES)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Preprocess audio function\n",
    "def preprocess_audio(file_path):\n",
    "    # Load the audio file and resample to 16kHz\n",
    "    waveform, sample_rate = torchaudio.load(file_path)\n",
    "    if sample_rate != 16000:\n",
    "        resample = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "        waveform = resample(waveform)\n",
    "    return waveform.squeeze().numpy()\n",
    "\n",
    "# Predict emotion function\n",
    "def predict_emotion_with_attention(audio_path):\n",
    "    # Preprocess audio\n",
    "    input_audio = preprocess_audio(audio_path)\n",
    "    \n",
    "    # Tokenize audio and prepare inputs\n",
    "    inputs = processor(input_audio, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "    input_values = inputs.input_values.to(device)\n",
    "    attention_mask = inputs.attention_mask.to(device)\n",
    "    \n",
    "    # Predict emotion\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values, attention_mask=attention_mask)\n",
    "        predicted_class = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    # Emotion labels (can customize based on dataset)\n",
    "    emotion_labels = {0: \"neutral\", 1: \"happy\", 2: \"sad\", 3: \"angry\", 4: \"fearful\", 5: \"disgust\", 6: \"surprised\", 7: \"calm\"}\n",
    "    return emotion_labels.get(predicted_class, \"unknown\")\n",
    "\n",
    "# Example usage\n",
    "audio_file = \"/home/common/ai_workspace/user_space/hooda_workspace/emotion_recognition/wave2vec2/audio_file/03-01-05-01-01-01-05.wav\"  # Replace with your audio file path\n",
    "# preprocess_audio(audio_file)\n",
    "emotion = predict_emotion_with_attention(preprocess_audio(audio_file))\n",
    "print(f\"Predicted Emotion: {emotion}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dad7da-c5a4-4126-8928-a4dd08fbbe2a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "emotion_rec_env",
   "language": "python",
   "name": "emotion_rec_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
